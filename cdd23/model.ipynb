{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "489fde99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.12.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (585.9 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 585.9/585.9 MB 513.3 kB/s eta 0:00:00\n",
      "Collecting tensorflow-estimator<2.13,>=2.12.0\n",
      "  Using cached tensorflow_estimator-2.12.0-py2.py3-none-any.whl (440 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.54.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.1/5.1 MB 6.4 MB/s eta 0:00:00\n",
      "Collecting jax>=0.3.15\n",
      "  Using cached jax-0.4.10.tar.gz (1.3 MB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Collecting tensorboard<2.13,>=2.12\n",
      "  Using cached tensorboard-2.12.3-py3-none-any.whl (5.6 MB)\n",
      "Collecting keras<2.13,>=2.12.0\n",
      "  Using cached keras-2.12.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Collecting six>=1.12.0\n",
      "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
      "Collecting packaging\n",
      "  Using cached packaging-23.1-py3-none-any.whl (48 kB)\n",
      "Collecting typing-extensions>=3.6.6\n",
      "  Using cached typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-67.7.2-py3-none-any.whl (1.1 MB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting numpy<1.24,>=1.22\n",
      "  Downloading numpy-1.23.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.1/17.1 MB 6.5 MB/s eta 0:00:00\n",
      "Collecting wrapt<1.15,>=1.11.0\n",
      "  Downloading wrapt-1.14.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (77 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.9/77.9 KB 4.5 MB/s eta 0:00:00\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 2.4/2.4 MB 6.9 MB/s eta 0:00:00\n",
      "Collecting h5py>=2.9.0\n",
      "  Downloading h5py-3.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 4.6/4.6 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting flatbuffers>=2.0\n",
      "  Using cached flatbuffers-23.5.9-py2.py3-none-any.whl (26 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3\n",
      "  Using cached protobuf-4.23.0-cp37-abi3-manylinux2014_x86_64.whl (304 kB)\n",
      "Collecting wheel<1.0,>=0.23.0\n",
      "  Using cached wheel-0.40.0-py3-none-any.whl (64 kB)\n",
      "Collecting scipy>=1.7\n",
      "  Downloading scipy-1.10.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 34.4/34.4 MB 6.6 MB/s eta 0:00:00\n",
      "Collecting ml-dtypes>=0.1.0\n",
      "  Downloading ml_dtypes-0.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (190 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 190.6/190.6 KB 6.8 MB/s eta 0:00:00\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.3.4-py3-none-any.whl (242 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "Collecting requests<3,>=2.21.0\n",
      "  Using cached requests-2.30.0-py3-none-any.whl (62 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Using cached google_auth-2.18.0-py2.py3-none-any.whl (178 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Using cached tensorboard_data_server-0.7.0-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting urllib3<2.0\n",
      "  Using cached urllib3-1.26.15-py2.py3-none-any.whl (140 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Collecting idna<4,>=2.5\n",
      "  Using cached idna-3.4-py3-none-any.whl (61 kB)\n",
      "Collecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (199 kB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.3/199.3 KB 8.1 MB/s eta 0:00:00\n",
      "Collecting certifi>=2017.4.17\n",
      "  Using cached certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
      "Collecting MarkupSafe>=2.1.1\n",
      "  Downloading MarkupSafe-2.1.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Using cached pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "Collecting oauthlib>=3.0.0\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Building wheels for collected packages: jax\n",
      "  Building wheel for jax (pyproject.toml): started\n",
      "  Building wheel for jax (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for jax: filename=jax-0.4.10-py3-none-any.whl size=1480517 sha256=3875ef1c33d70995420e3da23c521c5c47b560a25c13b0260f2b1ab4eea78d8a\n",
      "  Stored in directory: /home/cpdl/.cache/pip/wheels/2f/04/51/ebc9c5225f0a0df1e56c231c1f4c9b7afd3e024ebb492eed99\n",
      "Successfully built jax\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, wheel, urllib3, typing-extensions, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, six, setuptools, pyasn1, protobuf, packaging, oauthlib, numpy, MarkupSafe, markdown, keras, idna, grpcio, gast, charset-normalizer, certifi, cachetools, absl-py, werkzeug, scipy, rsa, requests, pyasn1-modules, opt-einsum, ml-dtypes, h5py, google-pasta, astunparse, requests-oauthlib, jax, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mysql-connector-python 8.0.31 requires protobuf<=3.20.1,>=3.11.0, but you have protobuf 4.23.0 which is incompatible.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully installed MarkupSafe-2.1.2 absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 certifi-2023.5.7 charset-normalizer-3.1.0 flatbuffers-23.5.9 gast-0.4.0 google-auth-2.18.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.54.0 h5py-3.8.0 idna-3.4 jax-0.4.10 keras-2.12.0 libclang-16.0.0 markdown-3.4.3 ml-dtypes-0.1.0 numpy-1.24.1 oauthlib-3.2.2 opt-einsum-3.3.0 packaging-23.1 protobuf-4.23.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-2.30.0 requests-oauthlib-1.3.1 rsa-4.9 scipy-1.10.1 setuptools-67.7.2 six-1.16.0 tensorboard-2.12.3 tensorboard-data-server-0.7.0 tensorflow-2.12.0 tensorflow-estimator-2.12.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 typing-extensions-4.5.0 urllib3-1.26.15 werkzeug-2.3.4 wheel-0.40.0 wrapt-1.14.1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "cmd = 'pip install tensorflow --ignore-installed'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "deebd2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.8 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 61.8/61.8 MB 6.2 MB/s eta 0:00:00\n",
      "Collecting numpy>=1.17.3\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 17.3/17.3 MB 6.0 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy, opencv-python\n",
      "Successfully installed numpy-1.24.3 opencv-python-4.7.0.72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.24.3 which is incompatible.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cmd = 'pip install opencv-python --ignore-installed'\n",
    "os.system(cmd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f534f1aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 13:16:30.675738: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-05-12 13:16:31.324897: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-05-12 13:16:31.324977: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-05-12 13:16:33.155687: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-12 13:16:33.155959: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-05-12 13:16:33.155985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "# importing libraries\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "# import seaborn as sns\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dense, Flatten, ZeroPadding2D, GlobalAveragePooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1f638a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameters\n",
    "BATCH_SIZE = 32\n",
    "IMG_WIDTH, IMG_HEIGHT = 64, 64\n",
    "\n",
    "TRAIN_DATA_PATH = './split_mel_test_train/train/'\n",
    "TEST_DATA_PATH = './split_mel_test_train/test'\n",
    "VALID_DATA_PATH = './split_mel_test_train/val'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea5c97c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_generator_from_dir(directory_path, batch_size_):\n",
    "    data_gen = ImageDataGenerator(rescale=1./255)\n",
    "    data_batch_generator = data_gen.flow_from_directory(\n",
    "        directory_path, color_mode = 'rgb', target_size = (IMG_WIDTH, IMG_HEIGHT), \n",
    "        batch_size=batch_size_)\n",
    "    \n",
    "    return data_batch_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b56cd55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1686 images belonging to 3 classes.\n",
      "Found 210 images belonging to 3 classes.\n",
      "Found 214 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "train_data = get_generator_from_dir(TRAIN_DATA_PATH, BATCH_SIZE)\n",
    "validation_data = get_generator_from_dir(VALID_DATA_PATH, BATCH_SIZE)\n",
    "test_data = get_generator_from_dir(TEST_DATA_PATH, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1051c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_layer = tf.keras.layers.LeakyReLU(alpha=0.1)\n",
    "visible = Input(shape=(IMG_WIDTH,IMG_HEIGHT,3),name='ip')\n",
    "conv1 = Conv2D(128, kernel_size=3, strides=(1,1), padding=\"valid\", activation=activation_layer,name='cn1')(visible)\n",
    "pool1 = MaxPooling2D(pool_size=(3, 3),strides=(2,2), padding=\"valid\",name='pl1')(conv1)\n",
    "\n",
    "conv2 = Conv2D(64, kernel_size=5, strides=(2,2), padding=\"valid\", activation=activation_layer,name='cn2')(pool1)\n",
    "pool2 = MaxPooling2D(pool_size=(3, 3),strides=(2,2), padding=\"valid\",name='pl2')(conv2)\n",
    "\n",
    "flat = Flatten(name='flat')(pool2)\n",
    "hidden1 = Dense(32, activation=activation_layer,name='hd1')(flat)\n",
    "output = Dense(3, activation='softmax',name='op')(hidden1)\n",
    "model = Model(inputs=visible, outputs=output)\n",
    "\n",
    "# Optimizers, loss function and performance metrics\n",
    "opt = tf.keras.optimizers.Adam(learning_rate=1e-4)\n",
    "cat_loss = tf.keras.losses.CategoricalCrossentropy()\n",
    "acc_metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "\n",
    "# Compiling the model\n",
    "model.compile(optimizer=opt, loss=cat_loss, metrics=acc_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f17f3b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " ip (InputLayer)             [(None, 64, 64, 3)]       0         \n",
      "                                                                 \n",
      " cn1 (Conv2D)                (None, 62, 62, 128)       3584      \n",
      "                                                                 \n",
      " pl1 (MaxPooling2D)          (None, 30, 30, 128)       0         \n",
      "                                                                 \n",
      " cn2 (Conv2D)                (None, 13, 13, 64)        204864    \n",
      "                                                                 \n",
      " pl2 (MaxPooling2D)          (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flat (Flatten)              (None, 2304)              0         \n",
      "                                                                 \n",
      " hd1 (Dense)                 (None, 32)                73760     \n",
      "                                                                 \n",
      " op (Dense)                  (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 282,307\n",
      "Trainable params: 282,307\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7bf7f0d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-12 13:27:12.245026: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 62980096 exceeds 10% of free system memory.\n",
      "2023-05-12 13:27:12.326439: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 62980096 exceeds 10% of free system memory.\n",
      "2023-05-12 13:27:12.353234: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 14745600 exceeds 10% of free system memory.\n",
      "2023-05-12 13:27:12.415579: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 14745600 exceeds 10% of free system memory.\n",
      "2023-05-12 13:27:12.415625: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 23795200 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53/53 [==============================] - 25s 437ms/step - loss: 0.7197 - categorical_accuracy: 0.6714 - val_loss: 0.5698 - val_categorical_accuracy: 0.6762\n",
      "Epoch 2/10\n",
      "53/53 [==============================] - 21s 406ms/step - loss: 0.4960 - categorical_accuracy: 0.7722 - val_loss: 0.4150 - val_categorical_accuracy: 0.8143\n",
      "Epoch 3/10\n",
      "53/53 [==============================] - 20s 368ms/step - loss: 0.3641 - categorical_accuracy: 0.8547 - val_loss: 0.3339 - val_categorical_accuracy: 0.8571\n",
      "Epoch 4/10\n",
      "53/53 [==============================] - 19s 360ms/step - loss: 0.2869 - categorical_accuracy: 0.8808 - val_loss: 0.2525 - val_categorical_accuracy: 0.9095\n",
      "Epoch 5/10\n",
      "53/53 [==============================] - 20s 376ms/step - loss: 0.2277 - categorical_accuracy: 0.9122 - val_loss: 0.2064 - val_categorical_accuracy: 0.9333\n",
      "Epoch 6/10\n",
      "53/53 [==============================] - 19s 359ms/step - loss: 0.1770 - categorical_accuracy: 0.9389 - val_loss: 0.1775 - val_categorical_accuracy: 0.9143\n",
      "Epoch 7/10\n",
      "53/53 [==============================] - 19s 357ms/step - loss: 0.1420 - categorical_accuracy: 0.9567 - val_loss: 0.1545 - val_categorical_accuracy: 0.9238\n",
      "Epoch 8/10\n",
      "53/53 [==============================] - 19s 355ms/step - loss: 0.1252 - categorical_accuracy: 0.9609 - val_loss: 0.1464 - val_categorical_accuracy: 0.9286\n",
      "Epoch 9/10\n",
      "53/53 [==============================] - 19s 361ms/step - loss: 0.1180 - categorical_accuracy: 0.9620 - val_loss: 0.1113 - val_categorical_accuracy: 0.9619\n",
      "Epoch 10/10\n",
      "53/53 [==============================] - 19s 357ms/step - loss: 0.0996 - categorical_accuracy: 0.9662 - val_loss: 0.0972 - val_categorical_accuracy: 0.9762\n"
     ]
    }
   ],
   "source": [
    "model_train = model.fit(train_data, batch_size = BATCH_SIZE, validation_data = validation_data, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "573fd73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 1s 107ms/step - loss: 0.0884 - categorical_accuracy: 0.9673\n"
     ]
    }
   ],
   "source": [
    "y = model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "520f859a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('cdd_model_2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d4be46bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Distress': 0, 'Egg': 1, 'Feed': 2}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f096b82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 187ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = tf.keras.preprocessing.image.load_img('split_mel_test_train/test/Distress/distress2-07.jpg', target_size=(IMG_WIDTH, IMG_HEIGHT))\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = np.expand_dims(x, axis=0)\n",
    "\n",
    "images = np.vstack([x])\n",
    "classes = model.predict(images, batch_size=BATCH_SIZE)\n",
    "classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
